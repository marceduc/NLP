{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CrossValidation.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"b2SZz8uDHI8_","colab_type":"code","outputId":"75993e8a-2191-4309-aac4-341cac0f06fe","executionInfo":{"status":"ok","timestamp":1546795081779,"user_tz":-60,"elapsed":4464,"user":{"displayName":"Marc Scheu","photoUrl":"","userId":"00403092945270721749"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n"],"name":"stdout"}]},{"metadata":{"id":"DjtSrjHld3x0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"f5a33b3b-3011-4190-aae8-a4982cd5f783","executionInfo":{"status":"ok","timestamp":1546883926194,"user_tz":-60,"elapsed":1117,"user":{"displayName":"Marc Scheu","photoUrl":"","userId":"00403092945270721749"}}},"cell_type":"code","source":["%cd 'drive/My Drive/IMDB/'"],"execution_count":7,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/IMDB\n"],"name":"stdout"}]},{"metadata":{"id":"-VhQr1AOHQG4","colab_type":"code","outputId":"da9971ce-7d31-468a-dfce-33c0fff40945","executionInfo":{"status":"ok","timestamp":1546883863674,"user_tz":-60,"elapsed":4746,"user":{"displayName":"Marc Scheu","photoUrl":"","userId":"00403092945270721749"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"cell_type":"code","source":["import os\n","import pandas as pd\n","import numpy as np\n","\n","from keras.models import Sequential, load_model\n","from keras.layers import Dense, Embedding,Bidirectional,GRU\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","from sklearn.model_selection import KFold\n","\n","import re\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","import nltk\n","import pickle\n","\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"KEGkseXTOGZw","colab_type":"code","colab":{}},"cell_type":"code","source":["def preprocess_reviews(reviews):\n","  # delete or replace special chars with whith spaces or End Of Sentence identifiers\n","  # remove stopwords and lemmanize\n","  # turn review strings into word list \n","\n","\n","  reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n","  reviews = [REPLACE_WITH_EOS.sub(\" EOS \", line) for line in reviews]\n","  reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n","\n","  for i in range(len(reviews)):\n","    review = reviews[i]\n","\n","    #lemmanize\n","    review = [lemmatizer.lemmatize(token) for token in review.split(\" \")]\n","    review = [lemmatizer.lemmatize(token, \"v\") for token in review]\n","\n","    #remove stopwords\n","    review = [word for word in review if not word in stop_words]\n","    review = \" \".join(review)\n","\n","    reviews[i] = review\n","\n","  return reviews\n","\n","def split_seq(s, y, SEQ_LEN):\n","  #creates for \n","  s_extend = sequence.pad_sequences([s[i:i+SEQ_LEN]  for i in range(0,len(s), SEQ_LEN)],  maxlen = SEQ_LEN)\n","  y_extend = np.repeat(y,s_extend.shape[0]).reshape(-1,1)\n","  l = s_extend.shape[0]\n","  \n","  return(s_extend, y_extend, l)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"UMLZBfJHXF5p","colab_type":"code","colab":{}},"cell_type":"code","source":["stop_words = set(stopwords.words(\"english\")) \n","lemmatizer = WordNetLemmatizer()\n","\n","REPLACE_NO_SPACE = re.compile(\"(\\;)|(\\:)|(\\')|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n","REPLACE_WITH_EOS = re.compile(\"(\\.)|(\\!)|(\\?)\")\n","REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")  \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"AJd5_wRoIQ_s","colab_type":"code","outputId":"88972e4d-1059-430a-9e7d-2ab1f2a7fc10","executionInfo":{"status":"ok","timestamp":1546883877349,"user_tz":-60,"elapsed":1671,"user":{"displayName":"Marc Scheu","photoUrl":"","userId":"00403092945270721749"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"cell_type":"code","source":["#PATH = 'drive/My Drive/IMDB/'\n","FILENAME = \"reviews_train.tsv\"\n","\n","FILEPATH =  FILENAME\n","FILEPATH"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'reviews_train.tsv'"]},"metadata":{"tags":[]},"execution_count":4}]},{"metadata":{"id":"rox6WLlgdz6L","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":82},"outputId":"e1239972-a79a-44b9-e0b6-d58002fed308","executionInfo":{"status":"ok","timestamp":1546883939554,"user_tz":-60,"elapsed":3769,"user":{"displayName":"Marc Scheu","photoUrl":"","userId":"00403092945270721749"}}},"cell_type":"code","source":["!ls"],"execution_count":8,"outputs":[{"output_type":"stream","text":["CrossValidation.ipynb  reviews_train.tsv  Uebung4-NotWorkingAsIntended.py\n","requirements.txt       tokenizer.pickle\n"],"name":"stdout"}]},{"metadata":{"id":"tw3vwQ6lH0sP","colab_type":"code","outputId":"f34d1a37-00bf-48b6-dca8-5363df8c0df6","colab":{"base_uri":"https://localhost:8080/","height":578}},"cell_type":"code","source":["## Cross Validation\n","\n","df = pd.read_table(FILEPATH, header = None,names = ['id', 'y','text'])\n","kf = KFold(n_splits=10, shuffle = True)\n","\n","MAX_VOCAB = 10000\n","SEQ_LEN = 70\n","\n","accs = []\n","\n","j = 0\n","for trn_idx, val_idx in kf.split(df):\n","\n","\n","\n","  ##training\n","  reviews_trn = df['text'][trn_idx]\n","  y_trn = (df.y.values == 'pos').astype('int')[trn_idx]\n","\n","  #preprocess and tokenize training data\n","  X_trn = preprocess_reviews(reviews_trn)\n","\n","  tokenizer = Tokenizer(num_words = MAX_VOCAB)\n","  tokenizer.fit_on_texts(X_trn)\n","  X_trn = tokenizer.texts_to_sequences(X_trn)\n","\n","\n","  #add observations by dividing reviews at SEQ_LEN in sub sequences\n","  y_long = np.empty([0,1], int)\n","  long_sequences  = np.empty([0,SEQ_LEN], int)\n","\n","\n","  for s, y in zip(X_trn, y_trn):\n","    s, y, _ = split_seq(s, y, SEQ_LEN)\n","    y_long = np.vstack([y_long, y])\n","    long_sequences = np.vstack([long_sequences, s])\n","\n","\n","  long_sequences.shape, y_long.shape\n","  X_trn = long_sequences\n","  y_trn = y_long\n","\n","  #construct model\n","  embedding_size= 100\n","  h_size = 32 \n","\n","  model=Sequential()\n","  model.add(Embedding(MAX_VOCAB, embedding_size, input_length=SEQ_LEN))\n","  model.add(Bidirectional(GRU(h_size, input_shape = (SEQ_LEN,embedding_size))))\n","  model.add(Dense(1, activation='sigmoid'))\n","  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","\n","  #train model for one epoch\n","  batch_size = 128\n","  epochs = 1\n","  model.fit(X_trn,y_trn, batch_size=batch_size, epochs=epochs)\n","\n","\n","\n","  ##validation\n","  reviews_val = df['text'][val_idx]\n","  y_val = (df.y.values == 'pos').astype('int')[val_idx]\n","\n","  X_val = preprocess_reviews(reviews_val)\n","  X_val = tokenizer.texts_to_sequences(X_val)\n","\n","  #prepare validation data\n","  len_ar = []\n","  X_long  = np.empty([0,SEQ_LEN], int)\n","\n","  for s, l in zip(X_val, y_val):\n","      s, _, l = split_seq(s, y, SEQ_LEN)\n","      X_long = np.vstack([X_long, s])\n","      len_ar.append(l)\n","\n","  len_ar = np.array(len_ar) \n","\n","  # make predictions\n","  pred = model.predict(X_long)\n","  predictions = []\n","  idx = 0\n","  for i in range(len_ar.shape[0]):\n","    y_hat = pred[idx: (idx+len_ar[i])].mean().round()\n","    predictions.append(y_hat)\n","    idx = idx + len_ar[i]\n","\n","  predictions = np.array(predictions)\n","  acc = (predictions == y_val).mean()\n","  accs.append(acc)\n","  print(\"Cross Fold: \" + str(j))\n","  print(\"Accuracy: \" +str(acc))\n","  j+=1"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/1\n","54287/54287 [==============================] - 213s 4ms/step - loss: 0.4631 - acc: 0.7698\n","Cross Fold: 0\n","Accuracy: 0.9\n","Epoch 1/1\n","54266/54266 [==============================] - 127s 2ms/step - loss: 0.4612 - acc: 0.7673\n","Cross Fold: 1\n","Accuracy: 0.8868\n","Epoch 1/1\n","54272/54272 [==============================] - 121s 2ms/step - loss: 0.4670 - acc: 0.7663\n","Cross Fold: 2\n","Accuracy: 0.8928\n","Epoch 1/1\n","54277/54277 [==============================] - 122s 2ms/step - loss: 0.4622 - acc: 0.7693\n","Cross Fold: 3\n","Accuracy: 0.8936\n","Epoch 1/1\n","54197/54197 [==============================] - 122s 2ms/step - loss: 0.4673 - acc: 0.7648\n","Cross Fold: 4\n","Accuracy: 0.8888\n","Epoch 1/1\n","54174/54174 [==============================] - 126s 2ms/step - loss: 0.4704 - acc: 0.7622\n","Cross Fold: 5\n","Accuracy: 0.892\n","Epoch 1/1\n","54205/54205 [==============================] - 126s 2ms/step - loss: 0.4609 - acc: 0.7710\n","Cross Fold: 6\n","Accuracy: 0.88\n","Epoch 1/1\n","54218/54218 [==============================] - 126s 2ms/step - loss: 0.4614 - acc: 0.7713\n","Cross Fold: 7\n","Accuracy: 0.8916\n","Epoch 1/1\n"],"name":"stdout"}]},{"metadata":{"id":"jzBwaIYDJNpG","colab_type":"code","colab":{}},"cell_type":"code","source":["accs = accs[0:10]\n","CV_res = accs\n","CV_res.append(np.mean(accs))\n","CV_res.append(np.std(accs))\n","CV_res.append(np.median(accs))\n","row_labels = [\"Fold\" + str(i) for i in range(10)] + ['Mean', 'Std', 'Median']\n","CV_res = pd.DataFrame(CV_res, columns = ['Accuracy'], index = row_labels)\n","CV_res"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TmnTn_ILJeCN","colab_type":"code","colab":{}},"cell_type":"code","source":["RESULTS_FILE = 'Cross_Validation_results.tsv'\n","RESULTS_PATH =  RESULTS_FILE\n","CV_res.to_csv(RESULTS_PATH,  sep = '\\t')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZbGeynzJlJDb","colab_type":"code","outputId":"89801377-f9be-42d2-c97e-ff3d255d490b","executionInfo":{"status":"ok","timestamp":1546681842636,"user_tz":-60,"elapsed":362836,"user":{"displayName":"Marc Scheu","photoUrl":"","userId":"00403092945270721749"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"cell_type":"code","source":["## Train\n","\n","#PATH = 'drive/My Drive/data/'\n","TRAIN_FILE = 'reviews_train.tsv'\n","\n","TRAIN_PATH =  TRAIN_FILE\n","\n","MODEL_NAME = 'GRU_BIDIREC.h5'\n","TOKENIZER_NAME = 'tokenizer.pickle'\n","\n","#input params\n","MAX_VOCAB = 10000\n","SEQ_LEN = 70\n","\n","#model params\n","embedding_size= 100\n","h_size = 32 \n","\n","#train params\n","batch_size = 128\n","epochs = 1\n","\n","#read data\n","df = pd.read_table(TRAIN_PATH, header = None,names = ['id', 'y','text'])\n","\n","##training\n","reviews_trn = df['text']\n","y_trn = (df.y.values == 'pos').astype('int')\n","\n","#preprocess and tokenize training data\n","X_trn = preprocess_reviews(reviews_trn)\n","\n","tokenizer = Tokenizer(num_words = MAX_VOCAB)\n","tokenizer.fit_on_texts(X_trn)\n","# save Tokenizer\n","with open(TOKENIZER_NAME, 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","\n","X_trn = tokenizer.texts_to_sequences(X_trn)\n","\n","\n","#add observations by dividing reviews at SEQ_LEN in sub sequences\n","y_long = np.empty([0,1], int)\n","long_sequences  = np.empty([0,SEQ_LEN], int)\n","\n","\n","for s, y in zip(X_trn, y_trn):\n","  s, y, _ = split_seq(s, y, SEQ_LEN)\n","  y_long = np.vstack([y_long, y])\n","  long_sequences = np.vstack([long_sequences, s])\n","\n","\n","long_sequences.shape, y_long.shape\n","X_trn = long_sequences\n","y_trn = y_long\n","\n","#construct model\n","model=Sequential()\n","model.add(Embedding(MAX_VOCAB, embedding_size, input_length=SEQ_LEN))\n","model.add(Bidirectional(GRU(h_size, input_shape = (SEQ_LEN,embedding_size))))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","#train model for one epoch\n","model.fit(X_trn,y_trn, batch_size=batch_size, epochs=epochs)\n","\n","model.save(MODEL_NAME)  # creates a HDF5 File MODELNAME\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/1\n","60249/60249 [==============================] - 166s 3ms/step - loss: 0.4592 - acc: 0.7703\n"],"name":"stdout"}]},{"metadata":{"id":"g5lCkD4BpDN5","colab_type":"code","colab":{}},"cell_type":"code","source":["del model  # deletes the existing model\n","del tokenizer\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Qv-ItwZxr76q","colab_type":"code","colab":{}},"cell_type":"code","source":["##Make Predictions\n","\n","#PATH = 'drive/My Drive/data/'\n","PREDICT_FILE = \"reviews_train.tsv\"\n","\n","PREDICT_PATH =  PREDICT_FILE\n","\n","MODEL_NAME = \"GRU_BIDIREC.h5\"\n","TOKENIZER_NAME = 'tokenizer.pickle'\n","\n","OUTPUT_FILE =  \"result_file.csv\"\n","\n","df = pd.read_table(PREDICT_PATH, header = None,names = ['id', 'y','text'])\n","\n","\n","#load tokenizer\n","with open(TOKENIZER_NAME, 'rb') as handle:\n","    tokenizer = pickle.load(handle)\n","#load model\n","model = load_model(MODEL_NAME)\n","\n","reviews_val = df['text']\n","\n","X_val = preprocess_reviews(reviews_val)\n","X_val = tokenizer.texts_to_sequences(X_val)\n","\n","#prepare validation data\n","len_ar = []\n","X_long  = np.empty([0,SEQ_LEN], int)\n","\n","for s in X_val:\n","    s, _, l = split_seq(s, 0, SEQ_LEN)\n","    X_long = np.vstack([X_long, s])\n","    len_ar.append(l)\n","\n","len_ar = np.array(len_ar) \n","\n","\n","# make predictions\n","pred = model.predict(X_long)\n","predictions = []\n","idx = 0\n","for i in range(len_ar.shape[0]):\n","  y_hat = pred[idx: (idx+len_ar[i])].mean().round()\n","  predictions.append(y_hat)\n","  idx = idx + len_ar[i]\n","\n","predictions = np.array(predictions)\n","output = pd.DataFrame(predictions, index = df['id'], columns = ['y_hat'])\n","output.replace([1.0,0.0], ['pos', 'neg'], inplace = True)\n","output.to_csv(OUTPUT_FILE, sep = '\\t')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Qj-uPSkyd68i","colab_type":"code","outputId":"d0ca8bf6-b904-4928-d16f-1ce92023cdcc","executionInfo":{"status":"ok","timestamp":1546682653525,"user_tz":-60,"elapsed":1500,"user":{"displayName":"Marc Scheu","photoUrl":"","userId":"00403092945270721749"}},"colab":{"base_uri":"https://localhost:8080/","height":99}},"cell_type":"code","source":["!ls 'drive/My Drive/data'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Cross_Validation_results2.csv\t NotWorkingAsIntended.py  review_51_cut.npy\n","Cross_Validation_results.csv\t requirements.txt\t  reviews_train.tsv\n","Cross_Validation_results.gsheet  result_file.csv\n"],"name":"stdout"}]}]}