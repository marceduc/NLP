{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Sentiment Prediction with Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4746,
     "status": "ok",
     "timestamp": 1546883863674,
     "user": {
      "displayName": "Marc Scheu",
      "photoUrl": "",
      "userId": "00403092945270721749"
     },
     "user_tz": -60
    },
    "id": "-VhQr1AOHQG4",
    "outputId": "da9971ce-7d31-468a-dfce-33c0fff40945"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Embedding,Bidirectional,GRU\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pickle\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KEGkseXTOGZw"
   },
   "outputs": [],
   "source": [
    "def preprocess_reviews(reviews):\n",
    "  # delete or replace special chars with whith spaces or End Of Sentence identifiers\n",
    "  # remove stopwords and lemmanize\n",
    "  # turn review strings into word list \n",
    "\n",
    "\n",
    "  reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n",
    "  reviews = [REPLACE_WITH_EOS.sub(\" EOS \", line) for line in reviews]\n",
    "  reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n",
    "\n",
    "  for i in range(len(reviews)):\n",
    "    review = reviews[i]\n",
    "\n",
    "    #lemmanize\n",
    "    review = [lemmatizer.lemmatize(token) for token in review.split(\" \")]\n",
    "    review = [lemmatizer.lemmatize(token, \"v\") for token in review]\n",
    "\n",
    "    #remove stopwords\n",
    "    review = [word for word in review if not word in stop_words]\n",
    "    review = \" \".join(review)\n",
    "\n",
    "    reviews[i] = review\n",
    "\n",
    "  return reviews\n",
    "\n",
    "def split_seq(s, y, SEQ_LEN):\n",
    "    #splits seuence s into subsequences of SEQ_LEN in list s_extend\n",
    "    #creates vector y_extend of the same length\n",
    "  s_extend = sequence.pad_sequences([s[i:i+SEQ_LEN]  for i in range(0,len(s), SEQ_LEN)],  maxlen = SEQ_LEN)\n",
    "  y_extend = np.repeat(y,s_extend.shape[0]).reshape(-1,1)\n",
    "  l = s_extend.shape[0]\n",
    "  \n",
    "  return(s_extend, y_extend, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UMLZBfJHXF5p"
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\")) \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# creating regular expression objects\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\;)|(\\:)|(\\')|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "REPLACE_WITH_EOS = re.compile(\"(\\.)|(\\!)|(\\?)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1671,
     "status": "ok",
     "timestamp": 1546883877349,
     "user": {
      "displayName": "Marc Scheu",
      "photoUrl": "",
      "userId": "00403092945270721749"
     },
     "user_tz": -60
    },
    "id": "AJd5_wRoIQ_s",
    "outputId": "88972e4d-1059-430a-9e7d-2ab1f2a7fc10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reviews_train.tsv'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PATH = 'drive/My Drive/IMDB/'\n",
    "FILENAME = \"reviews_train.tsv\"\n",
    "\n",
    "FILEPATH =  FILENAME\n",
    "FILEPATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train\n",
    "\n",
    "#PATH = 'drive/My Drive/data/'\n",
    "TRAIN_FILE = 'reviews_train.tsv'\n",
    "TRAIN_PATH =  TRAIN_FILE\n",
    "\n",
    "MODEL_NAME = 'GRU_BIDIREC.h5'\n",
    "TOKENIZER_NAME = 'tokenizer.pickle'\n",
    "\n",
    "#input params\n",
    "MAX_VOCAB = 10000\n",
    "SEQ_LEN = 70\n",
    "\n",
    "#read data\n",
    "df = pd.read_table(TRAIN_PATH, header = None,names = ['id', 'y','text'])\n",
    "\n",
    "##training\n",
    "reviews_trn = df['text']\n",
    "y_trn = (df.y.values == 'pos').astype('int')\n",
    "\n",
    "#preprocess and tokenize training data\n",
    "X_trn = preprocess_reviews(reviews_trn)\n",
    "\n",
    "tokenizer = Tokenizer(num_words = MAX_VOCAB)\n",
    "tokenizer.fit_on_texts(X_trn)\n",
    "# save Tokenizer\n",
    "with open(TOKENIZER_NAME, 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "X_trn = tokenizer.texts_to_sequences(X_trn)\n",
    "\n",
    "\n",
    "#add observations by dividing reviews at SEQ_LEN in sub sequences\n",
    "y_long = np.empty([0,1], int)\n",
    "long_sequences  = np.empty([0,SEQ_LEN], int)\n",
    "\n",
    "\n",
    "for s, y in zip(X_trn, y_trn):\n",
    "  s, y, _ = split_seq(s, y, SEQ_LEN)\n",
    "  y_long = np.vstack([y_long, y])\n",
    "  long_sequences = np.vstack([long_sequences, s])\n",
    "\n",
    "\n",
    "long_sequences.shape, y_long.shape\n",
    "X_trn = long_sequences\n",
    "y_trn = y_long\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Feedforward Network\n",
    "<img src=\"NN_simplified.png\" alt=\"NN\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Recurrent Neural Network\n",
    "<img src=\"RNN3.png\" alt=\"NN\" style=\"width: 500px;\"/>\n",
    "<BR><BR>\n",
    "<img src=\"RNN.png\" alt=\"NN\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension: Gated Recurrent Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"GRU.png\" alt=\"NN\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model params\n",
    "embedding_size= 100\n",
    "h_size = 32 \n",
    "\n",
    "#train params\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "\n",
    "#construct model\n",
    "model=Sequential()\n",
    "model.add(Embedding(MAX_VOCAB, embedding_size, input_length=SEQ_LEN))\n",
    "model.add(Bidirectional(GRU(h_size, input_shape = (SEQ_LEN,embedding_size))))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#train model for one epoch\n",
    "model.fit(X_trn,y_trn, batch_size=batch_size, epochs=epochs)\n",
    "\n",
    "model.save(MODEL_NAME)  # creates a HDF5 File MODELNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Make Predictions\n",
    "\n",
    "PREDICT_FILE = \"reviews_train.tsv\"\n",
    "PREDICT_PATH =  PREDICT_FILE\n",
    "OUTPUT_FILE =  \"result_file.csv\"\n",
    "\n",
    "MODEL_NAME = \"GRU_BIDIREC.h5\"\n",
    "TOKENIZER_NAME = 'tokenizer.pickle'\n",
    "\n",
    "df = pd.read_table(PREDICT_PATH, header = None,names = ['id', 'y','text'])\n",
    "\n",
    "#load tokenizer\n",
    "with open(TOKENIZER_NAME, 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "#load model\n",
    "model = load_model(MODEL_NAME)\n",
    "\n",
    "reviews_val = df['text']\n",
    "\n",
    "X_val = preprocess_reviews(reviews_val)\n",
    "X_val = tokenizer.texts_to_sequences(X_val)\n",
    "\n",
    "#prepare validation data\n",
    "len_ar = []\n",
    "X_long  = np.empty([0,SEQ_LEN], int)\n",
    "\n",
    "for s in X_val:\n",
    "    s, _, l = split_seq(s, 0, SEQ_LEN)\n",
    "    X_long = np.vstack([X_long, s])\n",
    "    len_ar.append(l)\n",
    "\n",
    "len_ar = np.array(len_ar) \n",
    "\n",
    "\n",
    "# make predictions\n",
    "pred = model.predict(X_long)\n",
    "predictions = []\n",
    "idx = 0\n",
    "for i in range(len_ar.shape[0]):\n",
    "  y_hat = pred[idx: (idx+len_ar[i])].mean().round()\n",
    "  predictions.append(y_hat)\n",
    "  idx = idx + len_ar[i]\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "output = pd.DataFrame(predictions, index = df['id'], columns = ['y_hat'])\n",
    "output.replace([1.0,0.0], ['pos', 'neg'], inplace = True)\n",
    "output.to_csv(OUTPUT_FILE, sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Mean</td>\n",
       "      <td>0.889280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Std</td>\n",
       "      <td>0.006352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Median</td>\n",
       "      <td>0.890440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Accuracy\n",
       "10       Mean  0.889280\n",
       "11        Std  0.006352\n",
       "12     Median  0.890440"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_res = pd.read_csv('Cross_Validation_results.tsv', sep='\\t')\n",
    "cv_res.tail(3)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CrossValidation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
